{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Model\n",
    "### Input: Audio Clip\n",
    "### Output: Time sections where pauses exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built from tutorial https://towardsdatascience.com/audio-deep-learning-made-simple-sound-classification-step-by-step-cebc936bbe5\n",
    "# Kaggle Dataset: https://www.kaggle.com/datasets/ikrbasak/sep-28k\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn import init\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import math, random\n",
    "import torch\n",
    "import torchaudio\n",
    "from torchaudio import transforms\n",
    "from IPython.display import Audio\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset, random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Dataframe with Dataset Files\n",
    "Removes .DS_Store and audio files with no length from the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             relative_path  NaturalPause\n",
      "0    FluencyBank_010_0.wav           0.0\n",
      "1    FluencyBank_010_1.wav           0.0\n",
      "2   FluencyBank_010_10.wav           0.0\n",
      "3   FluencyBank_010_11.wav           1.0\n",
      "4   FluencyBank_010_12.wav           0.0\n",
      "5   FluencyBank_010_13.wav           1.0\n",
      "6   FluencyBank_010_14.wav           0.0\n",
      "7   FluencyBank_010_15.wav           1.0\n",
      "8   FluencyBank_010_16.wav           0.0\n",
      "9   FluencyBank_010_17.wav           1.0\n",
      "10  FluencyBank_010_18.wav           1.0\n",
      "11  FluencyBank_010_19.wav           0.0\n",
      "12   FluencyBank_010_2.wav           1.0\n",
      "13  FluencyBank_010_20.wav           0.0\n",
      "14  FluencyBank_010_21.wav           1.0\n",
      "15  FluencyBank_010_22.wav           0.0\n",
      "16  FluencyBank_010_23.wav           1.0\n",
      "17  FluencyBank_010_24.wav           0.0\n",
      "18  FluencyBank_010_25.wav           0.0\n",
      "19  FluencyBank_010_26.wav           0.0\n"
     ]
    }
   ],
   "source": [
    "metadata_file = 'data/fluencybank_labels.csv'\n",
    "Y = pd.read_csv(metadata_file)\n",
    "Y = Y[['NaturalPause']]\n",
    "Y.columns =['NaturalPause']\n",
    "\n",
    "paths = []\n",
    "\n",
    "for i in listdir('data/clips'):\n",
    "    if isfile(join('data/clips', i)):\n",
    "        ## SKIPPING DS STORE\n",
    "        if not i == '.DS_Store':\n",
    "            if librosa.get_duration(filename=('./data/clips/' + i))>1:\n",
    "                paths.append(i)\n",
    "\n",
    "paths = sorted(paths)\n",
    "\n",
    "paths = pd.DataFrame(paths, columns = ['relative_path'])\n",
    "\n",
    "df = pd.concat([paths, Y], axis = 1)\n",
    "df = df[df['NaturalPause'].notnull()]\n",
    "print(df[:20])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio Pre-Processing and transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioUtil():\n",
    "  # ----------------------------\n",
    "  # Load an audio file. Return the signal as a tensor and the sample rate\n",
    "  # ----------------------------\n",
    "  @staticmethod\n",
    "  def open(audio_file):\n",
    "    sig, sr = torchaudio.load(audio_file)\n",
    "    return (sig, sr)\n",
    "\n",
    " # ----------------------------\n",
    "  # Convert the given audio to the desired number of channels\n",
    "  # ----------------------------\n",
    "  @staticmethod\n",
    "  def rechannel(aud, new_channel):\n",
    "    sig, sr = aud\n",
    "\n",
    "    if (sig.shape[0] == new_channel):\n",
    "      # Nothing to do\n",
    "      return aud\n",
    "\n",
    "    if (new_channel == 1):\n",
    "      # Convert from stereo to mono by selecting only the first channel\n",
    "      resig = sig[:1, :]\n",
    "    else:\n",
    "      # Convert from mono to stereo by duplicating the first channel\n",
    "      resig = torch.cat([sig, sig])\n",
    "\n",
    "    return ((resig, sr))\n",
    "# ----------------------------\n",
    "  # Since Resample applies to a single channel, we resample one channel at a time\n",
    "  # ----------------------------\n",
    "  @staticmethod\n",
    "  def resample(aud, newsr):\n",
    "    sig, sr = aud\n",
    "\n",
    "    if (sr == newsr):\n",
    "      # Nothing to do\n",
    "      return aud\n",
    "\n",
    "    num_channels = sig.shape[0]\n",
    "    # Resample first channel\n",
    "    resig = torchaudio.transforms.Resample(sr, newsr)(sig[:1,:])\n",
    "    if (num_channels > 1):\n",
    "      # Resample the second channel and merge both channels\n",
    "      retwo = torchaudio.transforms.Resample(sr, newsr)(sig[1:,:])\n",
    "      resig = torch.cat([resig, retwo])\n",
    "\n",
    "    return ((resig, newsr))\n",
    "\n",
    "      # ----------------------------\n",
    "  # Pad (or truncate) the signal to a fixed length 'max_ms' in milliseconds\n",
    "  # ----------------------------\n",
    "  @staticmethod\n",
    "  def pad_trunc(aud, max_ms):\n",
    "    sig, sr = aud\n",
    "    num_rows, sig_len = sig.shape\n",
    "    max_len = sr//1000 * max_ms\n",
    "\n",
    "    if (sig_len > max_len):\n",
    "      # Truncate the signal to the given length\n",
    "      sig = sig[:,:max_len]\n",
    "\n",
    "    elif (sig_len < max_len):\n",
    "      # Length of padding to add at the beginning and end of the signal\n",
    "      pad_begin_len = random.randint(0, max_len - sig_len)\n",
    "      pad_end_len = max_len - sig_len - pad_begin_len\n",
    "\n",
    "      # Pad with 0s\n",
    "      pad_begin = torch.zeros((num_rows, pad_begin_len))\n",
    "      pad_end = torch.zeros((num_rows, pad_end_len))\n",
    "\n",
    "      sig = torch.cat((pad_begin, sig, pad_end), 1)\n",
    "      \n",
    "    return (sig, sr)\n",
    "\n",
    "     # ----------------------------\n",
    "  # Shifts the signal to the left or right by some percent. Values at the end\n",
    "  # are 'wrapped around' to the start of the transformed signal.\n",
    "  # ----------------------------\n",
    "  @staticmethod\n",
    "  def time_shift(aud, shift_limit):\n",
    "    sig,sr = aud\n",
    "    _, sig_len = sig.shape\n",
    "    shift_amt = int(random.random() * shift_limit * sig_len)\n",
    "    return (sig.roll(shift_amt), sr)\n",
    "\n",
    "      # ----------------------------\n",
    "  # Generate a Spectrogram\n",
    "  # ----------------------------\n",
    "  @staticmethod\n",
    "  def spectro_gram(aud, n_mels=64, n_fft=1024, hop_len=None):\n",
    "    sig,sr = aud\n",
    "    top_db = 80\n",
    "\n",
    "    # spec has shape [channel, n_mels, time], where channel is mono, stereo etc\n",
    "    spec = transforms.MelSpectrogram(sr, n_fft=n_fft, hop_length=hop_len, n_mels=n_mels)(sig)\n",
    "\n",
    "    # Convert to decibels\n",
    "    spec = transforms.AmplitudeToDB(top_db=top_db)(spec)\n",
    "    return (spec)\n",
    "\n",
    "      # ----------------------------\n",
    "  # Augment the Spectrogram by masking out some sections of it in both the frequency\n",
    "  # dimension (ie. horizontal bars) and the time dimension (vertical bars) to prevent\n",
    "  # overfitting and to help the model generalise better. The masked sections are\n",
    "  # replaced with the mean value.\n",
    "  # ----------------------------\n",
    "  @staticmethod\n",
    "  def spectro_augment(spec, max_mask_pct=0.1, n_freq_masks=1, n_time_masks=1):\n",
    "    _, n_mels, n_steps = spec.shape\n",
    "    mask_value = spec.mean()\n",
    "    aug_spec = spec\n",
    "\n",
    "    freq_mask_param = max_mask_pct * n_mels\n",
    "    for _ in range(n_freq_masks):\n",
    "      aug_spec = transforms.FrequencyMasking(freq_mask_param)(aug_spec, mask_value)\n",
    "\n",
    "    time_mask_param = max_mask_pct * n_steps\n",
    "    for _ in range(n_time_masks):\n",
    "      aug_spec = transforms.TimeMasking(time_mask_param)(aug_spec, mask_value)\n",
    "\n",
    "    return aug_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Dataloader Dataset Class\n",
    "SoundDS is a class which the Dataloader iterates to sequence data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Sound Dataset\n",
    "# ----------------------------\n",
    "class SoundDS(Dataset):\n",
    "  def __init__(self, df, data_path):\n",
    "    self.df = df\n",
    "    self.data_path = str(data_path)\n",
    "    self.duration = 4000\n",
    "    self.sr = 44100\n",
    "    self.channel = 2\n",
    "    self.shift_pct = 0.4\n",
    "            \n",
    "  # ----------------------------\n",
    "  # Number of items in dataset\n",
    "  # ----------------------------\n",
    "  def __len__(self):\n",
    "    return len(self.df)    \n",
    "    \n",
    "  # ----------------------------\n",
    "  # Get i'th item in dataset\n",
    "  # ----------------------------\n",
    "  def __getitem__(self, idx):\n",
    "    # Absolute file path of the audio file - concatenate the audio directory with\n",
    "    # the relative path\n",
    "    audio_file = self.data_path + self.df.loc[idx, 'relative_path']\n",
    "    # Get the Class ID\n",
    "    naturalPause = self.df.loc[idx, 'NaturalPause']\n",
    "\n",
    "    aud = AudioUtil.open(audio_file)\n",
    "    # Some sounds have a higher sample rate, or fewer channels compared to the\n",
    "    # majority. So make all sounds have the same number of channels and same \n",
    "    # sample rate. Unless the sample rate is the same, the pad_trunc will still\n",
    "    # result in arrays of different lengths, even though the sound duration is\n",
    "    # the same.\n",
    "    reaud = AudioUtil.resample(aud, self.sr)\n",
    "\n",
    "    rechan = AudioUtil.rechannel(reaud, self.channel)\n",
    "    # dur_aud = AudioUtil.pad_trunc(rechan, self.duration)\n",
    "    \n",
    "    dur_aud = AudioUtil.pad_trunc(rechan, self.duration)\n",
    "    shift_aud = AudioUtil.time_shift(dur_aud, self.shift_pct)\n",
    "    sgram = AudioUtil.spectro_gram(shift_aud, n_mels=64, n_fft=1024, hop_len=None)\n",
    "    aug_sgram = AudioUtil.spectro_augment(sgram, max_mask_pct=0.1, n_freq_masks=2, n_time_masks=2)\n",
    "\n",
    "    return aug_sgram, naturalPause\n",
    "\n",
    "myds = SoundDS(df, './data/clips/')\n",
    "\n",
    "# Random split of 80:20 between training and validation\n",
    "num_items = len(myds)\n",
    "num_train = round(num_items * 0.8)\n",
    "num_val = num_items - num_train\n",
    "train_ds, val_ds = random_split(myds, [num_train, num_val])\n",
    "\n",
    "# Create training and validation data loaders\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "val_dl = torch.utils.data.DataLoader(val_ds, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and DataLoader Testing\n",
    "### Todo: Also implement test functions later down the line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataTest():\n",
    "    assert myds[2][0].shape == torch.Size([2, 64, 344])\n",
    "DataTest()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert myds[2][0].shape == torch.Size([2, 64, 344])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., dtype=torch.float64)\n",
      "tensor(0., dtype=torch.float64)\n",
      "tensor(0., dtype=torch.float64)\n",
      "tensor(0., dtype=torch.float64)\n",
      "tensor(0., dtype=torch.float64)\n",
      "tensor(0., dtype=torch.float64)\n",
      "tensor(0., dtype=torch.float64)\n",
      "tensor(1., dtype=torch.float64)\n",
      "tensor(0., dtype=torch.float64)\n",
      "tensor(0., dtype=torch.float64)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/tagsabi/pause_removal_tool/model.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/tagsabi/pause_removal_tool/model.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, data \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_dl):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tagsabi/pause_removal_tool/model.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mprint\u001b[39m(data[\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m])\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    682\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    720\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 721\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    723\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataset.py:290\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(idx, \u001b[39mlist\u001b[39m):\n\u001b[1;32m    289\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m idx]]\n\u001b[0;32m--> 290\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices[idx]]\n",
      "\u001b[1;32m/Users/tagsabi/pause_removal_tool/model.ipynb Cell 13\u001b[0m in \u001b[0;36mSoundDS.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tagsabi/pause_removal_tool/model.ipynb#X15sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m aud \u001b[39m=\u001b[39m AudioUtil\u001b[39m.\u001b[39mopen(audio_file)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tagsabi/pause_removal_tool/model.ipynb#X15sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39m# Some sounds have a higher sample rate, or fewer channels compared to the\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tagsabi/pause_removal_tool/model.ipynb#X15sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39m# majority. So make all sounds have the same number of channels and same \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tagsabi/pause_removal_tool/model.ipynb#X15sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39m# sample rate. Unless the sample rate is the same, the pad_trunc will still\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tagsabi/pause_removal_tool/model.ipynb#X15sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39m# result in arrays of different lengths, even though the sound duration is\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tagsabi/pause_removal_tool/model.ipynb#X15sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39m# the same.\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/tagsabi/pause_removal_tool/model.ipynb#X15sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m reaud \u001b[39m=\u001b[39m AudioUtil\u001b[39m.\u001b[39;49mresample(aud, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msr)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tagsabi/pause_removal_tool/model.ipynb#X15sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m rechan \u001b[39m=\u001b[39m AudioUtil\u001b[39m.\u001b[39mrechannel(reaud, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchannel)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tagsabi/pause_removal_tool/model.ipynb#X15sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39m# dur_aud = AudioUtil.pad_trunc(rechan, self.duration)\u001b[39;00m\n",
      "\u001b[1;32m/Users/tagsabi/pause_removal_tool/model.ipynb Cell 13\u001b[0m in \u001b[0;36mAudioUtil.resample\u001b[0;34m(aud, newsr)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tagsabi/pause_removal_tool/model.ipynb#X15sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m num_channels \u001b[39m=\u001b[39m sig\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tagsabi/pause_removal_tool/model.ipynb#X15sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39m# Resample first channel\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/tagsabi/pause_removal_tool/model.ipynb#X15sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m resig \u001b[39m=\u001b[39m torchaudio\u001b[39m.\u001b[39;49mtransforms\u001b[39m.\u001b[39;49mResample(sr, newsr)(sig[:\u001b[39m1\u001b[39m,:])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tagsabi/pause_removal_tool/model.ipynb#X15sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39mif\u001b[39;00m (num_channels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tagsabi/pause_removal_tool/model.ipynb#X15sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m   \u001b[39m# Resample the second channel and merge both channels\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tagsabi/pause_removal_tool/model.ipynb#X15sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m   retwo \u001b[39m=\u001b[39m torchaudio\u001b[39m.\u001b[39mtransforms\u001b[39m.\u001b[39mResample(sr, newsr)(sig[\u001b[39m1\u001b[39m:,:])\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/torch/lib/python3.9/site-packages/torchaudio/transforms/_transforms.py:937\u001b[0m, in \u001b[0;36mResample.__init__\u001b[0;34m(self, orig_freq, new_freq, resampling_method, lowpass_filter_width, rolloff, beta, dtype)\u001b[0m\n\u001b[1;32m    934\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbeta \u001b[39m=\u001b[39m beta\n\u001b[1;32m    936\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39morig_freq \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnew_freq:\n\u001b[0;32m--> 937\u001b[0m     kernel, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwidth \u001b[39m=\u001b[39m _get_sinc_resample_kernel(\n\u001b[1;32m    938\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49morig_freq,\n\u001b[1;32m    939\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnew_freq,\n\u001b[1;32m    940\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgcd,\n\u001b[1;32m    941\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlowpass_filter_width,\n\u001b[1;32m    942\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrolloff,\n\u001b[1;32m    943\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mresampling_method,\n\u001b[1;32m    944\u001b[0m         beta,\n\u001b[1;32m    945\u001b[0m         dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m    946\u001b[0m     )\n\u001b[1;32m    947\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mregister_buffer(\u001b[39m\"\u001b[39m\u001b[39mkernel\u001b[39m\u001b[39m\"\u001b[39m, kernel)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/torch/lib/python3.9/site-packages/torchaudio/functional/functional.py:1451\u001b[0m, in \u001b[0;36m_get_sinc_resample_kernel\u001b[0;34m(orig_freq, new_freq, gcd, lowpass_filter_width, rolloff, resampling_method, beta, device, dtype)\u001b[0m\n\u001b[1;32m   1448\u001b[0m idx \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39marange(\u001b[39m-\u001b[39mwidth, width \u001b[39m+\u001b[39m orig_freq, device\u001b[39m=\u001b[39mdevice, dtype\u001b[39m=\u001b[39midx_dtype)\n\u001b[1;32m   1450\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(new_freq):\n\u001b[0;32m-> 1451\u001b[0m     t \u001b[39m=\u001b[39m (\u001b[39m-\u001b[39;49mi \u001b[39m/\u001b[39;49m new_freq \u001b[39m+\u001b[39;49m idx \u001b[39m/\u001b[39;49m orig_freq) \u001b[39m*\u001b[39m base_freq\n\u001b[1;32m   1452\u001b[0m     t \u001b[39m=\u001b[39m t\u001b[39m.\u001b[39mclamp_(\u001b[39m-\u001b[39mlowpass_filter_width, lowpass_filter_width)\n\u001b[1;32m   1454\u001b[0m     \u001b[39m# we do not use built in torch windows here as we need to evaluate the window\u001b[39;00m\n\u001b[1;32m   1455\u001b[0m     \u001b[39m# at specific positions, not over a regular grid.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(train_dl):\n",
    "    print(data[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [-v] [-q] [--locals] [-f] [-c] [-b]\n",
      "                             [-k TESTNAMEPATTERNS]\n",
      "                             [tests ...]\n",
      "ipykernel_launcher.py: error: argument -f/--failfast: ignored explicit argument '/Users/tagsabi/Library/Jupyter/runtime/kernel-v2-59817T4SUEYtj9YM4.json'\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArgumentError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/miniconda3/envs/torch/lib/python3.9/argparse.py:1857\u001b[0m, in \u001b[0;36mArgumentParser.parse_known_args\u001b[0;34m(self, args, namespace)\u001b[0m\n\u001b[1;32m   1856\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1857\u001b[0m     namespace, args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_parse_known_args(args, namespace)\n\u001b[1;32m   1858\u001b[0m \u001b[39mexcept\u001b[39;00m ArgumentError:\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/torch/lib/python3.9/argparse.py:2066\u001b[0m, in \u001b[0;36mArgumentParser._parse_known_args\u001b[0;34m(self, arg_strings, namespace)\u001b[0m\n\u001b[1;32m   2065\u001b[0m     \u001b[39m# consume the next optional and any arguments for it\u001b[39;00m\n\u001b[0;32m-> 2066\u001b[0m     start_index \u001b[39m=\u001b[39m consume_optional(start_index)\n\u001b[1;32m   2068\u001b[0m \u001b[39m# consume any positionals following the last Optional\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/torch/lib/python3.9/argparse.py:1988\u001b[0m, in \u001b[0;36mArgumentParser._parse_known_args.<locals>.consume_optional\u001b[0;34m(start_index)\u001b[0m\n\u001b[1;32m   1987\u001b[0m         msg \u001b[39m=\u001b[39m _(\u001b[39m'\u001b[39m\u001b[39mignored explicit argument \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m-> 1988\u001b[0m         \u001b[39mraise\u001b[39;00m ArgumentError(action, msg \u001b[39m%\u001b[39m explicit_arg)\n\u001b[1;32m   1990\u001b[0m \u001b[39m# if there is no explicit argument, try to match the\u001b[39;00m\n\u001b[1;32m   1991\u001b[0m \u001b[39m# optional's string arguments with the following strings\u001b[39;00m\n\u001b[1;32m   1992\u001b[0m \u001b[39m# if successful, exit the loop\u001b[39;00m\n\u001b[1;32m   1993\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mArgumentError\u001b[0m: argument -f/--failfast: ignored explicit argument '/Users/tagsabi/Library/Jupyter/runtime/kernel-v2-59817T4SUEYtj9YM4.json'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[1;32m/Users/tagsabi/pause_removal_tool/model.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tagsabi/pause_removal_tool/model.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39munittest\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/tagsabi/pause_removal_tool/model.ipynb#X16sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mTestData\u001b[39;00m(unittest\u001b[39m.\u001b[39mTestCase):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tagsabi/pause_removal_tool/model.ipynb#X16sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mDatasetLabelTest\u001b[39m(\u001b[39mself\u001b[39m):\n",
      "\u001b[1;32m/Users/tagsabi/pause_removal_tool/model.ipynb Cell 14\u001b[0m in \u001b[0;36mTestData\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tagsabi/pause_removal_tool/model.ipynb#X16sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# if __name__ == '__main__':\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/tagsabi/pause_removal_tool/model.ipynb#X16sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     unittest\u001b[39m.\u001b[39;49mmain()\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/torch/lib/python3.9/unittest/main.py:100\u001b[0m, in \u001b[0;36mTestProgram.__init__\u001b[0;34m(self, module, defaultTest, argv, testRunner, testLoader, exit, verbosity, failfast, catchbreak, buffer, warnings, tb_locals)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprogName \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mbasename(argv[\u001b[39m0\u001b[39m])\n\u001b[0;32m--> 100\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparseArgs(argv)\n\u001b[1;32m    101\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunTests()\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/torch/lib/python3.9/unittest/main.py:133\u001b[0m, in \u001b[0;36mTestProgram.parseArgs\u001b[0;34m(self, argv)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 133\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_main_parser\u001b[39m.\u001b[39;49mparse_args(argv[\u001b[39m1\u001b[39;49m:], \u001b[39mself\u001b[39;49m)\n\u001b[1;32m    135\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtests:\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/torch/lib/python3.9/argparse.py:1824\u001b[0m, in \u001b[0;36mArgumentParser.parse_args\u001b[0;34m(self, args, namespace)\u001b[0m\n\u001b[1;32m   1823\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mparse_args\u001b[39m(\u001b[39mself\u001b[39m, args\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, namespace\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m-> 1824\u001b[0m     args, argv \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparse_known_args(args, namespace)\n\u001b[1;32m   1825\u001b[0m     \u001b[39mif\u001b[39;00m argv:\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/torch/lib/python3.9/argparse.py:1860\u001b[0m, in \u001b[0;36mArgumentParser.parse_known_args\u001b[0;34m(self, args, namespace)\u001b[0m\n\u001b[1;32m   1859\u001b[0m         err \u001b[39m=\u001b[39m _sys\u001b[39m.\u001b[39mexc_info()[\u001b[39m1\u001b[39m]\n\u001b[0;32m-> 1860\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merror(\u001b[39mstr\u001b[39;49m(err))\n\u001b[1;32m   1861\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/torch/lib/python3.9/argparse.py:2581\u001b[0m, in \u001b[0;36mArgumentParser.error\u001b[0;34m(self, message)\u001b[0m\n\u001b[1;32m   2580\u001b[0m args \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mprog\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprog, \u001b[39m'\u001b[39m\u001b[39mmessage\u001b[39m\u001b[39m'\u001b[39m: message}\n\u001b[0;32m-> 2581\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexit(\u001b[39m2\u001b[39;49m, _(\u001b[39m'\u001b[39;49m\u001b[39m%(prog)s\u001b[39;49;00m\u001b[39m: error: \u001b[39;49m\u001b[39m%(message)s\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m'\u001b[39;49m) \u001b[39m%\u001b[39;49m args)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/torch/lib/python3.9/argparse.py:2568\u001b[0m, in \u001b[0;36mArgumentParser.exit\u001b[0;34m(self, status, message)\u001b[0m\n\u001b[1;32m   2567\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_print_message(message, _sys\u001b[39m.\u001b[39mstderr)\n\u001b[0;32m-> 2568\u001b[0m _sys\u001b[39m.\u001b[39;49mexit(status)\n",
      "\u001b[0;31mSystemExit\u001b[0m: 2",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/torch/lib/python3.9/site-packages/IPython/core/interactiveshell.py:1983\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   1980\u001b[0m \u001b[39mif\u001b[39;00m exception_only:\n\u001b[1;32m   1981\u001b[0m     stb \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mAn exception has occurred, use \u001b[39m\u001b[39m%\u001b[39m\u001b[39mtb to see \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   1982\u001b[0m            \u001b[39m'\u001b[39m\u001b[39mthe full traceback.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m]\n\u001b[0;32m-> 1983\u001b[0m     stb\u001b[39m.\u001b[39mextend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mInteractiveTB\u001b[39m.\u001b[39;49mget_exception_only(etype,\n\u001b[1;32m   1984\u001b[0m                                                      value))\n\u001b[1;32m   1985\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1986\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1987\u001b[0m         \u001b[39m# Exception classes can customise their traceback - we\u001b[39;00m\n\u001b[1;32m   1988\u001b[0m         \u001b[39m# use this in IPython.parallel for exceptions occurring\u001b[39;00m\n\u001b[1;32m   1989\u001b[0m         \u001b[39m# in the engines. This should return a list of strings.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/torch/lib/python3.9/site-packages/IPython/core/ultratb.py:585\u001b[0m, in \u001b[0;36mListTB.get_exception_only\u001b[0;34m(self, etype, value)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_exception_only\u001b[39m(\u001b[39mself\u001b[39m, etype, value):\n\u001b[1;32m    578\u001b[0m     \u001b[39m\"\"\"Only print the exception type and message, without a traceback.\u001b[39;00m\n\u001b[1;32m    579\u001b[0m \n\u001b[1;32m    580\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[39m    value : exception value\u001b[39;00m\n\u001b[1;32m    584\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 585\u001b[0m     \u001b[39mreturn\u001b[39;00m ListTB\u001b[39m.\u001b[39;49mstructured_traceback(\u001b[39mself\u001b[39;49m, etype, value)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/torch/lib/python3.9/site-packages/IPython/core/ultratb.py:443\u001b[0m, in \u001b[0;36mListTB.structured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, context)\u001b[0m\n\u001b[1;32m    440\u001b[0m     chained_exc_ids\u001b[39m.\u001b[39madd(\u001b[39mid\u001b[39m(exception[\u001b[39m1\u001b[39m]))\n\u001b[1;32m    441\u001b[0m     chained_exceptions_tb_offset \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    442\u001b[0m     out_list \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 443\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstructured_traceback(\n\u001b[1;32m    444\u001b[0m             etype, evalue, (etb, chained_exc_ids),\n\u001b[1;32m    445\u001b[0m             chained_exceptions_tb_offset, context)\n\u001b[1;32m    446\u001b[0m         \u001b[39m+\u001b[39m chained_exception_message\n\u001b[1;32m    447\u001b[0m         \u001b[39m+\u001b[39m out_list)\n\u001b[1;32m    449\u001b[0m \u001b[39mreturn\u001b[39;00m out_list\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/torch/lib/python3.9/site-packages/IPython/core/ultratb.py:1118\u001b[0m, in \u001b[0;36mAutoFormattedTB.structured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1116\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1117\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtb \u001b[39m=\u001b[39m tb\n\u001b[0;32m-> 1118\u001b[0m \u001b[39mreturn\u001b[39;00m FormattedTB\u001b[39m.\u001b[39;49mstructured_traceback(\n\u001b[1;32m   1119\u001b[0m     \u001b[39mself\u001b[39;49m, etype, value, tb, tb_offset, number_of_lines_of_context)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/torch/lib/python3.9/site-packages/IPython/core/ultratb.py:1012\u001b[0m, in \u001b[0;36mFormattedTB.structured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1009\u001b[0m mode \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode\n\u001b[1;32m   1010\u001b[0m \u001b[39mif\u001b[39;00m mode \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose_modes:\n\u001b[1;32m   1011\u001b[0m     \u001b[39m# Verbose modes need a full traceback\u001b[39;00m\n\u001b[0;32m-> 1012\u001b[0m     \u001b[39mreturn\u001b[39;00m VerboseTB\u001b[39m.\u001b[39;49mstructured_traceback(\n\u001b[1;32m   1013\u001b[0m         \u001b[39mself\u001b[39;49m, etype, value, tb, tb_offset, number_of_lines_of_context\n\u001b[1;32m   1014\u001b[0m     )\n\u001b[1;32m   1015\u001b[0m \u001b[39melif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mMinimal\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m   1016\u001b[0m     \u001b[39mreturn\u001b[39;00m ListTB\u001b[39m.\u001b[39mget_exception_only(\u001b[39mself\u001b[39m, etype, value)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/torch/lib/python3.9/site-packages/IPython/core/ultratb.py:865\u001b[0m, in \u001b[0;36mVerboseTB.structured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m    856\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstructured_traceback\u001b[39m(\n\u001b[1;32m    857\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    858\u001b[0m     etype: \u001b[39mtype\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    862\u001b[0m     number_of_lines_of_context: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m,\n\u001b[1;32m    863\u001b[0m ):\n\u001b[1;32m    864\u001b[0m     \u001b[39m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 865\u001b[0m     formatted_exception \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mformat_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[1;32m    866\u001b[0m                                                            tb_offset)\n\u001b[1;32m    868\u001b[0m     colors \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mColors  \u001b[39m# just a shorthand + quicker name lookup\u001b[39;00m\n\u001b[1;32m    869\u001b[0m     colorsnormal \u001b[39m=\u001b[39m colors\u001b[39m.\u001b[39mNormal  \u001b[39m# used a lot\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/torch/lib/python3.9/site-packages/IPython/core/ultratb.py:799\u001b[0m, in \u001b[0;36mVerboseTB.format_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m    796\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(tb_offset, \u001b[39mint\u001b[39m)\n\u001b[1;32m    797\u001b[0m head \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_header(etype, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlong_header)\n\u001b[1;32m    798\u001b[0m records \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 799\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_records(etb, number_of_lines_of_context, tb_offset) \u001b[39mif\u001b[39;00m etb \u001b[39melse\u001b[39;00m []\n\u001b[1;32m    800\u001b[0m )\n\u001b[1;32m    802\u001b[0m frames \u001b[39m=\u001b[39m []\n\u001b[1;32m    803\u001b[0m skipped \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/torch/lib/python3.9/site-packages/IPython/core/ultratb.py:854\u001b[0m, in \u001b[0;36mVerboseTB.get_records\u001b[0;34m(self, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m    848\u001b[0m     formatter \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    849\u001b[0m options \u001b[39m=\u001b[39m stack_data\u001b[39m.\u001b[39mOptions(\n\u001b[1;32m    850\u001b[0m     before\u001b[39m=\u001b[39mbefore,\n\u001b[1;32m    851\u001b[0m     after\u001b[39m=\u001b[39mafter,\n\u001b[1;32m    852\u001b[0m     pygments_formatter\u001b[39m=\u001b[39mformatter,\n\u001b[1;32m    853\u001b[0m )\n\u001b[0;32m--> 854\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39;49m(stack_data\u001b[39m.\u001b[39;49mFrameInfo\u001b[39m.\u001b[39;49mstack_data(etb, options\u001b[39m=\u001b[39;49moptions))[tb_offset:]\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/torch/lib/python3.9/site-packages/stack_data/core.py:544\u001b[0m, in \u001b[0;36mFrameInfo.stack_data\u001b[0;34m(cls, frame_or_tb, options, collapse_repeated_frames)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    529\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstack_data\u001b[39m(\n\u001b[1;32m    530\u001b[0m         \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    534\u001b[0m         collapse_repeated_frames: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    535\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Iterator[Union[\u001b[39m'\u001b[39m\u001b[39mFrameInfo\u001b[39m\u001b[39m'\u001b[39m, RepeatedFrames]]:\n\u001b[1;32m    536\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    537\u001b[0m \u001b[39m    An iterator of FrameInfo and RepeatedFrames objects representing\u001b[39;00m\n\u001b[1;32m    538\u001b[0m \u001b[39m    a full traceback or stack. Similar consecutive frames are collapsed into RepeatedFrames\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[39m    and optionally an Options object to configure.\u001b[39;00m\n\u001b[1;32m    543\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 544\u001b[0m     stack \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(iter_stack(frame_or_tb))\n\u001b[1;32m    546\u001b[0m     \u001b[39m# Reverse the stack from a frame so that it's in the same order\u001b[39;00m\n\u001b[1;32m    547\u001b[0m     \u001b[39m# as the order from a traceback, which is the order of a printed\u001b[39;00m\n\u001b[1;32m    548\u001b[0m     \u001b[39m# traceback when read top to bottom (most recent call last)\u001b[39;00m\n\u001b[1;32m    549\u001b[0m     \u001b[39mif\u001b[39;00m is_frame(frame_or_tb):\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/torch/lib/python3.9/site-packages/stack_data/utils.py:98\u001b[0m, in \u001b[0;36miter_stack\u001b[0;34m(frame_or_tb)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[39mwhile\u001b[39;00m frame_or_tb:\n\u001b[1;32m     97\u001b[0m     \u001b[39myield\u001b[39;00m frame_or_tb\n\u001b[0;32m---> 98\u001b[0m     \u001b[39mif\u001b[39;00m is_frame(frame_or_tb):\n\u001b[1;32m     99\u001b[0m         frame_or_tb \u001b[39m=\u001b[39m frame_or_tb\u001b[39m.\u001b[39mf_back\n\u001b[1;32m    100\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/torch/lib/python3.9/site-packages/stack_data/utils.py:91\u001b[0m, in \u001b[0;36mis_frame\u001b[0;34m(frame_or_tb)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mis_frame\u001b[39m(frame_or_tb: Union[FrameType, TracebackType]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mbool\u001b[39m:\n\u001b[0;32m---> 91\u001b[0m     assert_(\u001b[39misinstance\u001b[39;49m(frame_or_tb, (types\u001b[39m.\u001b[39;49mFrameType, types\u001b[39m.\u001b[39;49mTracebackType)))\n\u001b[1;32m     92\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39misinstance\u001b[39m(frame_or_tb, (types\u001b[39m.\u001b[39mFrameType,))\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/torch/lib/python3.9/site-packages/stack_data/utils.py:172\u001b[0m, in \u001b[0;36massert_\u001b[0;34m(condition, error)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(error, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    171\u001b[0m     error \u001b[39m=\u001b[39m \u001b[39mAssertionError\u001b[39;00m(error)\n\u001b[0;32m--> 172\u001b[0m \u001b[39mraise\u001b[39;00m error\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "class TestData(unittest.TestCase):\n",
    "\n",
    "    def DatasetLabelTest(self):\n",
    "        for i, data in enumerate(train_dl):\n",
    "            self.assertEqual(data[1][0], torch.Size([]))\n",
    "        \n",
    "\n",
    "    def DatasetInputTest(self):\n",
    "        for i, data in enumerate(train_dl):\n",
    "            self.assertEqual(data[0][0], torch.Size([2, 64, 344]))\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "    unittest.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., dtype=torch.float64)\n",
      "tensor(0., dtype=torch.float64)\n",
      "tensor(1., dtype=torch.float64)\n",
      "tensor(0., dtype=torch.float64)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/tagsabi/pause_removal_tool/model.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/tagsabi/pause_removal_tool/model.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, data \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_dl):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tagsabi/pause_removal_tool/model.ipynb#X20sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mprint\u001b[39m(data[\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m])\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    682\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    720\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 721\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    723\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataset.py:290\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(idx, \u001b[39mlist\u001b[39m):\n\u001b[1;32m    289\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m idx]]\n\u001b[0;32m--> 290\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices[idx]]\n",
      "\u001b[1;32m/Users/tagsabi/pause_removal_tool/model.ipynb Cell 15\u001b[0m in \u001b[0;36mSoundDS.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tagsabi/pause_removal_tool/model.ipynb#X20sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m aud \u001b[39m=\u001b[39m AudioUtil\u001b[39m.\u001b[39mopen(audio_file)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tagsabi/pause_removal_tool/model.ipynb#X20sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39m# Some sounds have a higher sample rate, or fewer channels compared to the\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tagsabi/pause_removal_tool/model.ipynb#X20sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39m# majority. So make all sounds have the same number of channels and same \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tagsabi/pause_removal_tool/model.ipynb#X20sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39m# sample rate. Unless the sample rate is the same, the pad_trunc will still\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tagsabi/pause_removal_tool/model.ipynb#X20sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39m# result in arrays of different lengths, even though the sound duration is\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tagsabi/pause_removal_tool/model.ipynb#X20sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39m# the same.\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/tagsabi/pause_removal_tool/model.ipynb#X20sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m reaud \u001b[39m=\u001b[39m AudioUtil\u001b[39m.\u001b[39;49mresample(aud, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msr)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tagsabi/pause_removal_tool/model.ipynb#X20sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m rechan \u001b[39m=\u001b[39m AudioUtil\u001b[39m.\u001b[39mrechannel(reaud, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchannel)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tagsabi/pause_removal_tool/model.ipynb#X20sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39m# dur_aud = AudioUtil.pad_trunc(rechan, self.duration)\u001b[39;00m\n",
      "\u001b[1;32m/Users/tagsabi/pause_removal_tool/model.ipynb Cell 15\u001b[0m in \u001b[0;36mAudioUtil.resample\u001b[0;34m(aud, newsr)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tagsabi/pause_removal_tool/model.ipynb#X20sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m num_channels \u001b[39m=\u001b[39m sig\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tagsabi/pause_removal_tool/model.ipynb#X20sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39m# Resample first channel\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/tagsabi/pause_removal_tool/model.ipynb#X20sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m resig \u001b[39m=\u001b[39m torchaudio\u001b[39m.\u001b[39;49mtransforms\u001b[39m.\u001b[39;49mResample(sr, newsr)(sig[:\u001b[39m1\u001b[39m,:])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tagsabi/pause_removal_tool/model.ipynb#X20sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39mif\u001b[39;00m (num_channels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tagsabi/pause_removal_tool/model.ipynb#X20sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m   \u001b[39m# Resample the second channel and merge both channels\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tagsabi/pause_removal_tool/model.ipynb#X20sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m   retwo \u001b[39m=\u001b[39m torchaudio\u001b[39m.\u001b[39mtransforms\u001b[39m.\u001b[39mResample(sr, newsr)(sig[\u001b[39m1\u001b[39m:,:])\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/torch/lib/python3.9/site-packages/torchaudio/transforms/_transforms.py:937\u001b[0m, in \u001b[0;36mResample.__init__\u001b[0;34m(self, orig_freq, new_freq, resampling_method, lowpass_filter_width, rolloff, beta, dtype)\u001b[0m\n\u001b[1;32m    934\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbeta \u001b[39m=\u001b[39m beta\n\u001b[1;32m    936\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39morig_freq \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnew_freq:\n\u001b[0;32m--> 937\u001b[0m     kernel, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwidth \u001b[39m=\u001b[39m _get_sinc_resample_kernel(\n\u001b[1;32m    938\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49morig_freq,\n\u001b[1;32m    939\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnew_freq,\n\u001b[1;32m    940\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgcd,\n\u001b[1;32m    941\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlowpass_filter_width,\n\u001b[1;32m    942\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrolloff,\n\u001b[1;32m    943\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mresampling_method,\n\u001b[1;32m    944\u001b[0m         beta,\n\u001b[1;32m    945\u001b[0m         dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m    946\u001b[0m     )\n\u001b[1;32m    947\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mregister_buffer(\u001b[39m\"\u001b[39m\u001b[39mkernel\u001b[39m\u001b[39m\"\u001b[39m, kernel)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/torch/lib/python3.9/site-packages/torchaudio/functional/functional.py:1457\u001b[0m, in \u001b[0;36m_get_sinc_resample_kernel\u001b[0;34m(orig_freq, new_freq, gcd, lowpass_filter_width, rolloff, resampling_method, beta, device, dtype)\u001b[0m\n\u001b[1;32m   1454\u001b[0m \u001b[39m# we do not use built in torch windows here as we need to evaluate the window\u001b[39;00m\n\u001b[1;32m   1455\u001b[0m \u001b[39m# at specific positions, not over a regular grid.\u001b[39;00m\n\u001b[1;32m   1456\u001b[0m \u001b[39mif\u001b[39;00m resampling_method \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msinc_interpolation\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m-> 1457\u001b[0m     window \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcos(t \u001b[39m*\u001b[39;49m math\u001b[39m.\u001b[39;49mpi \u001b[39m/\u001b[39m lowpass_filter_width \u001b[39m/\u001b[39m \u001b[39m2\u001b[39m) \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m\n\u001b[1;32m   1458\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1459\u001b[0m     \u001b[39m# kaiser_window\u001b[39;00m\n\u001b[1;32m   1460\u001b[0m     \u001b[39mif\u001b[39;00m beta \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(train_dl):\n",
    "    print(data[1][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 64, 344])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myds[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Audio Classification Model\n",
    "# ----------------------------\n",
    "class AudioClassifier(nn.Module):\n",
    "    # ----------------------------\n",
    "    # Build the model architecture\n",
    "    # ----------------------------\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        conv_layers = []\n",
    "\n",
    "        # First Convolution Block with Relu and Batch Norm. Use Kaiming Initialization\n",
    "        self.conv1 = nn.Conv2d(2, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.bn1 = nn.BatchNorm2d(8)\n",
    "        init.kaiming_normal_(self.conv1.weight, a=0.1)\n",
    "        self.conv1.bias.data.zero_()\n",
    "        conv_layers += [self.conv1, self.relu1, self.bn1]\n",
    "\n",
    "        # Second Convolution Block\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "        init.kaiming_normal_(self.conv2.weight, a=0.1)\n",
    "        self.conv2.bias.data.zero_()\n",
    "        conv_layers += [self.conv2, self.relu2, self.bn2]\n",
    "\n",
    "        # Second Convolution Block\n",
    "        self.conv3 = nn.Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        init.kaiming_normal_(self.conv3.weight, a=0.1)\n",
    "        self.conv3.bias.data.zero_()\n",
    "        conv_layers += [self.conv3, self.relu3, self.bn3]\n",
    "\n",
    "        # Second Convolution Block\n",
    "        self.conv4 = nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        init.kaiming_normal_(self.conv4.weight, a=0.1)\n",
    "        self.conv4.bias.data.zero_()\n",
    "        conv_layers += [self.conv4, self.relu4, self.bn4]\n",
    "\n",
    "        # Linear Classifier\n",
    "        self.ap = nn.AdaptiveAvgPool2d(output_size=1)\n",
    "        self.lin = nn.Linear(in_features=64, out_features=1)\n",
    "\n",
    "        # Wrap the Convolutional Blocks\n",
    "        self.conv = nn.Sequential(*conv_layers)\n",
    " \n",
    "    # ----------------------------\n",
    "    # Forward pass computations\n",
    "    # ----------------------------\n",
    "    def forward(self, x):\n",
    "        # Run the convolutional blocks\n",
    "        x = self.conv(x)\n",
    "\n",
    "        # Adaptive pool and flatten for input to linear layer\n",
    "        x = self.ap(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "\n",
    "        # Linear layer\n",
    "        x = self.lin(x)\n",
    "\n",
    "        # Final output\n",
    "        return x\n",
    "\n",
    "# Create the model and put it on the GPU if available\n",
    "myModel = AudioClassifier()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "myModel = myModel.to(device)\n",
    "# Check that it is on Cuda\n",
    "next(myModel.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7w/glnww6954kb5vp_bchm9g9dw0000gq/T/ipykernel_60132/4057533726.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs = torch.tensor(inputs, dtype=torch.float32)\n",
      "/var/folders/7w/glnww6954kb5vp_bchm9g9dw0000gq/T/ipykernel_60132/4057533726.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels, dtype=torch.float32)\n",
      "/Users/tagsabi/opt/miniconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/loss.py:530: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/tagsabi/pause_removal_tool/model.ipynb Cell 21\u001b[0m in \u001b[0;36m<cell line: 79>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tagsabi/pause_removal_tool/model.ipynb#X26sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m   \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mFinished Training\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tagsabi/pause_removal_tool/model.ipynb#X26sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m num_epochs\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m   \u001b[39m# Just for demo, adjust this higher.\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/tagsabi/pause_removal_tool/model.ipynb#X26sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m training(myModel, train_dl, num_epochs)\n",
      "\u001b[1;32m/Users/tagsabi/pause_removal_tool/model.ipynb Cell 21\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(model, train_dl, num_epochs)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tagsabi/pause_removal_tool/model.ipynb#X26sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m total_prediction \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tagsabi/pause_removal_tool/model.ipynb#X26sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# Repeat for each batch in the training set\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/tagsabi/pause_removal_tool/model.ipynb#X26sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, data \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_dl):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tagsabi/pause_removal_tool/model.ipynb#X26sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     \u001b[39m# Get the input features and target labels, and put them on the GPU\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tagsabi/pause_removal_tool/model.ipynb#X26sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tagsabi/pause_removal_tool/model.ipynb#X26sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     \u001b[39m# inputs, labels = data[0].to(device), data[1].to(device)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tagsabi/pause_removal_tool/model.ipynb#X26sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tagsabi/pause_removal_tool/model.ipynb#X26sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     \u001b[39m# print(torch.tensor(data, dtype=torch.float))\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tagsabi/pause_removal_tool/model.ipynb#X26sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     inputs, labels \u001b[39m=\u001b[39m data[\u001b[39m0\u001b[39m], data[\u001b[39m1\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tagsabi/pause_removal_tool/model.ipynb#X26sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     inputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(inputs, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    682\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    720\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 721\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    723\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataset.py:290\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(idx, \u001b[39mlist\u001b[39m):\n\u001b[1;32m    289\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m idx]]\n\u001b[0;32m--> 290\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices[idx]]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# ----------------------------\n",
    "# Training Loop\n",
    "# ----------------------------\n",
    "def training(model, train_dl, num_epochs):\n",
    "  # Loss Function, Optimizer and Scheduler\n",
    "  criterion = nn.MSELoss()\n",
    "  # criterion = nn.MSELoss()\n",
    "  optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "  scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001,\n",
    "                                                steps_per_epoch=int(len(train_dl)),\n",
    "                                                epochs=num_epochs,\n",
    "                                                anneal_strategy='linear')\n",
    "\n",
    "  # Repeat for each epoch\n",
    "  for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    correct_prediction = 0\n",
    "    total_prediction = 0\n",
    "\n",
    "    # Repeat for each batch in the training set\n",
    "    for i, data in enumerate(train_dl):\n",
    "        # Get the input features and target labels, and put them on the GPU\n",
    "        \n",
    "        # inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        # print(torch.tensor(data, dtype=torch.float))\n",
    "\n",
    "        inputs, labels = data[0], data[1]\n",
    "\n",
    "        inputs = torch.tensor(inputs, dtype=torch.float32)\n",
    "        labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "        # Normalize the inputs\n",
    "        inputs_m, inputs_s = inputs.mean(), inputs.std()\n",
    "        inputs = (inputs - inputs_m) / inputs_s\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        # print(outputs.dtype)\n",
    "        # print(inputs.dtype)\n",
    "\n",
    "        # print(f' Outputs: {outputs}')\n",
    "        # print(f' Inputs: {inputs}')\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # Keep stats for Loss and Accuracy\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Get the predicted class with the highest score\n",
    "        _, prediction = torch.max(outputs,1)\n",
    "        # Count of predictions that matched the target label\n",
    "        correct_prediction += (prediction == labels).sum().item()\n",
    "        total_prediction += prediction.shape[0]\n",
    "\n",
    "        #if i % 10 == 0:    # print every 10 mini-batches\n",
    "        #    print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 10))\n",
    "    \n",
    "    # Print stats at the end of the epoch\n",
    "    num_batches = len(train_dl)\n",
    "    avg_loss = running_loss / num_batches\n",
    "    acc = correct_prediction/total_prediction\n",
    "    print(f'Epoch: {epoch}, Loss: {avg_loss:.2f}, Accuracy: {acc:.2f}')\n",
    "\n",
    "  print('Finished Training')\n",
    "\n",
    "num_epochs=5   # Just for demo, adjust this higher.\n",
    "training(myModel, train_dl, num_epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(myModel, \"model.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "283a34aeaf43db022676d6a711ff12072d0ab8971ee5eb0695094659d4b27294"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
